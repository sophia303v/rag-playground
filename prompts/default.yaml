# General-Purpose RAG - Prompt Templates (Domain-Agnostic)
# This template contains prompts that work across any knowledge domain.
# To create a domain-specific template, copy this file and customize accordingly.

# --- Generation Prompts ---

system_prompt: |
  You are a knowledgeable AI assistant. Your role is to answer questions
  based on retrieved reference documents.

  IMPORTANT RULES:
  1. Only answer based on the provided reference documents.
  2. If the reference documents don't contain relevant information, say so clearly.
  3. Always cite which source(s) you used in your answer (e.g., [Source 1], [Source 2]).
  4. Be accurate, concise, and well-organized.
  5. If an image is provided, integrate your image analysis with the retrieved references.

user_prompt: |
  Based on the following reference documents from our knowledge base,
  please answer the user's question.

  --- RETRIEVED REFERENCES ---
  {context}
  --- END REFERENCES ---

  User Question: {query}

user_prompt_image_suffix: |

  Image Analysis: The uploaded image shows: {image_description}

  Please integrate the image findings with the reference documents to provide a comprehensive answer.

disclaimer: "\n\n---\n*Disclaimer: This information is for reference purposes only. Please verify with authoritative sources before making decisions.*"

# --- Retriever Prompts ---

image_description_prompt: >
  Describe the key content and details visible in this image.
  Focus on notable elements, structure, and any important observations.
  Be concise but thorough.

# --- Evaluation Prompts ---

faithfulness_prompt: |
  You are an evaluation judge for a RAG system.

  Given the CONTEXT (retrieved documents), QUESTION, and ANSWER below,
  evaluate whether the ANSWER is faithful to the CONTEXT.

  An answer is faithful if every factual claim it makes can be traced back
  to information in the context. The answer should not hallucinate facts
  that are not in the context.

  CONTEXT:
  {context}

  QUESTION:
  {question}

  ANSWER:
  {answer}

  Respond in JSON format:
  {{
    "score": <float between 0.0 and 1.0>,
    "explanation": "<brief explanation of your score>"
  }}

  Scoring guide:
  - 1.0: Every claim in the answer is supported by the context
  - 0.7-0.9: Most claims are supported, minor unsupported details
  - 0.4-0.6: Mix of supported and unsupported claims
  - 0.1-0.3: Mostly unsupported claims
  - 0.0: Answer contradicts the context or is entirely fabricated

answer_relevancy_prompt: |
  You are an evaluation judge for a RAG system.

  Given the QUESTION and ANSWER below, evaluate whether the ANSWER
  is relevant to the QUESTION.

  A relevant answer directly addresses what was asked, stays on topic,
  and provides the type of information the question is seeking.

  QUESTION:
  {question}

  ANSWER:
  {answer}

  Respond in JSON format:
  {{
    "score": <float between 0.0 and 1.0>,
    "explanation": "<brief explanation of your score>"
  }}

  Scoring guide:
  - 1.0: Answer directly and completely addresses the question
  - 0.7-0.9: Answer mostly addresses the question with minor tangents
  - 0.4-0.6: Answer partially addresses the question
  - 0.1-0.3: Answer is mostly off-topic
  - 0.0: Answer is completely irrelevant to the question

judge_prompt: |
  You are a domain expert evaluating an AI assistant's answer.

  Given the QUESTION, GROUND TRUTH answer, AI ANSWER, and the RETRIEVED CONTEXT,
  score the AI answer on three criteria.

  QUESTION:
  {question}

  GROUND TRUTH (reference answer):
  {ground_truth}

  AI ANSWER:
  {answer}

  RETRIEVED CONTEXT:
  {context}

  Score each criterion from 0.0 to 1.0:

  1. **Domain Appropriateness** (0-1): Does the answer use correct terminology
     for the subject matter? Is the information accurate and appropriate?
     - 1.0: Terminology and accuracy are excellent
     - 0.5: Some terminology issues or minor inaccuracies
     - 0.0: Seriously incorrect information

  2. **Citation Accuracy** (0-1): Does the answer cite its sources (e.g., [Source 1])?
     Do the citations match the actual content of the retrieved documents?
     - 1.0: All claims are properly cited with correct source references
     - 0.5: Some citations present but incomplete or partially incorrect
     - 0.0: No citations or completely wrong citations

  3. **Answer Completeness** (0-1): Does the answer cover the key points from
     the ground truth? Are important details mentioned?
     - 1.0: All key points from ground truth are covered
     - 0.5: Some key points covered, some missing
     - 0.0: Major key points missing entirely

  Respond in JSON format only:
  {{
    "domain_appropriateness": <float>,
    "citation_accuracy": <float>,
    "answer_completeness": <float>,
    "explanation": "<brief justification covering all three criteria>"
  }}

combined_eval_prompt: |
  You are an evaluation judge for a RAG system. Evaluate the AI ANSWER on ALL five criteria below in a single assessment.

  QUESTION:
  {question}

  GROUND TRUTH (reference answer):
  {ground_truth}

  AI ANSWER:
  {answer}

  RETRIEVED CONTEXT:
  {context}

  Score each criterion from 0.0 to 1.0:

  1. **Faithfulness**: Is every factual claim in the answer supported by the context?
     - 1.0: Every claim is supported  - 0.5: Mix of supported/unsupported  - 0.0: Contradicts context or fabricated

  2. **Answer Relevancy**: Does the answer directly address the question?
     - 1.0: Directly and completely addresses the question  - 0.5: Partially addresses  - 0.0: Completely irrelevant

  3. **Domain Appropriateness**: Does the answer use correct terminology? Is it accurate?
     - 1.0: Excellent terminology and accuracy  - 0.5: Minor issues  - 0.0: Seriously incorrect

  4. **Citation Accuracy**: Does the answer cite sources and do citations match the context?
     - 1.0: All claims properly cited  - 0.5: Incomplete citations  - 0.0: No or wrong citations

  5. **Answer Completeness**: Does the answer cover key points from the ground truth?
     - 1.0: All key points covered  - 0.5: Some missing  - 0.0: Major points missing

  Respond in JSON format only:
  {{
    "faithfulness": <float>,
    "answer_relevancy": <float>,
    "domain_appropriateness": <float>,
    "citation_accuracy": <float>,
    "answer_completeness": <float>,
    "explanation": "<brief justification covering all five criteria>"
  }}

# --- QA Generation Prompts ---

factual_qa_prompt: |
  Given this document, generate {n} QA pairs in JSON format.
  Each QA pair must test factual understanding of the document content.

  Document ID: {uid}
  Content: {findings}
  Summary: {impression}

  Requirements:
  - Questions should ask about specific facts, details, or observations
  - Answers must be directly derivable from the document text
  - Vary question types: what, where, who, how, describe, etc.
  - Keep answers concise but complete (1-3 sentences)

  Return ONLY a JSON array (no markdown fences):
  [
    {{
      "question": "...",
      "ground_truth_answer": "...",
      "category": "factual",
      "difficulty": "easy|medium|hard"
    }}
  ]

comparative_qa_prompt: |
  Given these documents, generate {n} QA pairs that compare content across documents.

  {reports_block}

  Requirements:
  - Questions should compare or contrast information between 2-3 documents
  - Reference documents by their ID numbers
  - Answers must be derivable from the provided documents
  - Examples: "How does the conclusion in document X differ from document Y?"

  Return ONLY a JSON array (no markdown fences):
  [
    {{
      "question": "...",
      "ground_truth_answer": "...",
      "relevant_report_uids": ["uid1", "uid2"],
      "category": "comparative",
      "difficulty": "medium|hard"
    }}
  ]

diagnostic_qa_prompt: |
  Given this document, generate {n} QA pairs focused on reasoning and analysis.

  Document ID: {uid}
  Content: {findings}
  Summary: {impression}

  Requirements:
  - Questions should ask about significance, implications, or recommended actions
  - Answers must be derivable from the document text
  - Examples: "What conclusion is supported by the findings?", "What action is recommended?"

  Return ONLY a JSON array (no markdown fences):
  [
    {{
      "question": "...",
      "ground_truth_answer": "...",
      "category": "diagnostic",
      "difficulty": "medium|hard"
    }}
  ]
